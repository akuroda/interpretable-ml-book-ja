
<!--{pagebreak}-->

<!--
## Counterfactual Explanations {#counterfactual}
-->
## 反事実的説明 (Counterfactual Explanations) {#反事実的}

```{r, message = FALSE, warning = FALSE, echo = FALSE}
devtools::load_all()
set.seed(42)
```

<!--
A counterfactual explanation describes a causal situation in the form: "If X had not occurred, Y would not have occurred".
For example: "If I hadn't taken a sip of this hot coffee, I wouldn't have burned my tongue".
Event Y is that I burned my tongue;
cause X is that I had a hot coffee.
Thinking in counterfactuals requires imagining a hypothetical reality that contradicts the observed facts (e.g. a world in which I have not drunk the hot coffee), hence the name "counterfactual".
The ability to think in counterfactuals makes us humans so smart compared to other animals.
-->
反事実的説明は、「もしXが起こらなかったら、Yも起こらなかっただろう」のような因果的状況で述べられます。
「このホットコーヒーを一口飲まなければ、舌を火傷しなかっただろう」という例では、
イベント Y は舌を火傷したことであり、原因 X はホットコーヒーを飲んだこととなります。
反事実を考えることは、観察された事実と矛盾する（例えば、ホットコーヒーを飲まなかったという世界のような）仮説的な現実を想像する必要があります。それ故に、「反事実的」という名称がつけられているのです。
反事実を考える能力によって、我々人類は他の動物と比較してとても賢くなりました。

<!--
In interpretable machine learning, counterfactual explanations can be used to explain predictions of individual instances.
The "event" is the predicted outcome of an instance, the "causes" are the particular feature values of this instance that were input to the model and "caused" a certain prediction.
Displayed as a graph, the relationship between the inputs and the prediction is very simple:
The feature values cause the prediction.
-->
解釈可能な機械学習において、反事実的説明は個々の事象に対する予測を説明する際に使用されます。
「出来事」はあるインスタンスの予測結果であり、「原因」は、モデルに入力されたモデルに入力された予測の要因となるインスタンスのある特徴量の値です。
グラフが示すように、入力と予測の関係性はとても単純で、特徴量の値が予測の原因となっています。

<!--
fig.cap = "The causal relationships between inputs of a machine learning model and the predictions, when the model is merely seen as a black box. The inputs cause the prediction (not necessarily reflecting the real causal relation of the data)."
-->

```{r ml-graph-cf, fig.cap = "機械学習モデルの入力と予測の因果関係。モデルは単にブラックボックスモデルとみなされる。入力が予測の原因となっている（ただし、データの現実の因果を反映している必要はない）", out.width=500}
knitr::include_graphics("images/graph.jpg")
```

<!--
Even if in reality the relationship between the inputs and the outcome to be predicted might not be causal, we can see the inputs of a model as the cause of the prediction.
-->
たとえ入力と予測される結果の関係性が現実には因果関係でなかったとしても、モデルの入力を予測の原因みなすことができます。

<!--
Given this simple graph, it is easy to see how we can simulate counterfactuals for predictions of machine learning models:
We simply change the feature values of an instance before making the predictions and we analyze how the prediction changes.
We are interested in scenarios in which the prediction changes in a relevant way, like a flip in predicted class (e.g. credit application accepted or rejected) or in which the prediction reaches a certain threshold (e.g. the probability for cancer reaches 10%).
**A counterfactual explanation of a prediction describes the smallest change to the feature values that changes the prediction to a predefined output.**
-->
この単純なグラフが与えられたとき、機械学習モデルの予測に対して、反事実を想定する方法は簡単です。
予測をする前にインスタンスの特徴量の値を変化させ、どのように予測が変化するかを分析します。
予測されたクラスの反転 (例: クレジットカード申請の可否の反転) や、予測がある閾値に到達する (例: がんの確率が 10% に到達する) など、関連する方法で予測が変化するシナリオに関心があります。
**予測の反事実的な説明は、最初に定義された出力に予測を変化させるような特徴量の最小の変更方法を示します**。

<!--
The counterfactual explanation method is model-agnostic, since it only works with the model inputs and output.
This method would also feel at home in the [model-agnostic chapter](#agnostic), since the interpretation can be expressed as a summary of the differences in feature values ("change features A and B to change the prediction").
But a counterfactual explanation is itself a new instance, so it lives in this chapter ("starting from instance X, change A and B to get a counterfactual instance").
Unlike [prototypes](#proto), counterfactuals do not have to be actual instances from the training data, but can be a new combination of feature values.
-->
反事実的説明はモデルの入力と出力のみを用いるため、モデル非依存の手法です。
この手法が生み出す解釈は特徴量の違いを要約したもの（特徴量 A と特徴量 B を変更することで予測を変えることができる）とも捉えることができるため、この手法は[モデル非依存の手法（model-agnostic methods）の章](#agnostic)で取り上げることもできたでしょう。
しかし、反事実的説明はそれ自体が新しいインスタンスであるため、この章で取り上げています（インスタンス X から始め、A と B を変更することで反事実的説明を得る）。
[プロトタイプ（prototypes)](#proto)とは異なり、反事実は学習データに実際に含まれるインスタンスである必要はありませんが、特徴量の新しい組み合わせである可能性はあります。

<!--
Before discussing how to create counterfactuals, I would like to discuss some use cases for counterfactuals and how a good counterfactual explanation looks like.
-->
反事実の作成方法を紹介する前に、いくつかの反事実の使用例と良い反事実的説明がどのようなものなのかということについて説明します。

<!--
In this first example, Peter applies for a loan and gets rejected by the (machine learning powered) banking software.
He wonders why his application was rejected and how he might improve his chances to get a loan.
The question of "why" can be formulated as a counterfactual:
What is the smallest change to the features (income, number of credit cards, age, ...) that would change the prediction from rejected to approved?
One possible answer could be:
If Peter would earn 10,000 Euro more per year, he would get the loan.
Or if Peter had fewer credit cards and had not defaulted on a loan 5 years ago, he would get the loan.
Peter will never know the reasons for the rejection, as the bank has no interest in transparency, but that is another story.
-->
まず最初の例では、ある男性が、申請した融資を（機械学習を利用している）銀行のソフトウェアに却下された、という状況を考えます。
彼はなぜ自分の申請が却下され、またどのようにしたら融資を得られる見込みが高くなるかということについて不思議に思いました。
この「なぜ」という疑問は、反事実を用いることによって明確にできます。
予測を却下から承認に変えるための特徴量（収入、クレジットカードの枚数、年齢など）の最小の変化はなんでしょうか。
この問いに対する答えには例えば次のようなものが考えられます。
この男性が年間 10,000 ユーロ以上を稼ぐならば、彼は融資を受けられたでしょう。
もしくは、この男性の持つクレジットカードがより少なく、5年前に不履行に陥っていなかったならば、融資を受けられたでしょう。
この銀行は透明性に関して全く関心を持っていなかったため男性が融資を断られた理由を知ることはありませんでしたが、それはまた別の話です。

<!--
In our second example we want to explain a model that predicts a continuous outcome with counterfactual explanations.
Anna wants to rent out her apartment, but she is not sure how much to charge for it, so she decides to train a machine learning model to predict the rent.
Of course, since Anna is a data scientist, that is how she solves her problems.
After entering all the details about size, location, whether pets are allowed and so on, the model tells her that she can charge 900 Euro.
She expected 1000 Euro or more, but she trusts her model and decides to play with the feature values of the apartment to see how she can improve the value of the apartment.
She finds out that the apartment could be rented out for over 1000 Euro, if it were 15 m^2^ larger.
Interesting, but non-actionable knowledge, because she cannot enlarge her apartment.
Finally, by tweaking only the feature values under her control (built-in kitchen yes/no, pets allowed yes/no, type of floor, etc.), she finds out that if she allows pets and installs windows with better insulation, she can charge 1000 Euro.
Anna had intuitively worked with counterfactuals to change the outcome.
-->
次の例では、連続的な結果を予測するモデルを反事実を用いて説明しようと思います。
ある女性が所有するアパートを貸し出したいと考えているが、家賃をどの程度にすればわからないため、家賃を予測するために機械学習モデルを学習することにした、という状況を考えます。
もちろん、この女性はデータサイエンティストであるため、これこそが彼女が問題を解決する手段です。
家の大きさ、立地、ペット可かどうかなどの詳細を全て入力すると、モデルは家賃を 900 ユーロと予測しました。
彼女は家賃は 1000 ユーロ以上になると考えていましたが、自分のモデルを信頼し、アパートの価値をどのようにしたら高めることができるかを見るために特徴量を変化させてみることにしました。
そして彼女は部屋が 15m^2^ よりも大きかった場合、1000 ユーロ以上で貸し出せる可能性があることを見つけました。
興味深い結果ですが、アパートを大きくできないため、これは実現不可能な知識です。
最終的には、彼女は自分でも改善できる特徴量の値（ビルトインキッチンがあるか、ペット可かどうか、床のタイプなど）を微調整することによって、ペットを許可し、より良い断熱窓を設置すれば家賃を1000ユーロ以上にできることを突き止めます。
彼女は直感的に反事実を利用することにより結果を変えたのです。

<!--
Counterfactuals are [human-friendly explanations](#good-explanation), because they are contrastive to the current instance and because they are selective, meaning they usually focus on a small number of feature changes.
But counterfactuals suffer from the 'Rashomon effect'.
Rashomon is a Japanese movie in which the murder of a Samurai is told by different people.
Each of the stories explains the outcome equally well, but the stories contradict each other.
The same can also happen with counterfactuals, since there are usually multiple different counterfactual explanations.
Each counterfactual tells a different "story" of how a certain outcome was reached.
One counterfactual might say to change feature A, the other counterfactual might say to leave A the same but change feature B, which is a contradiction.
This issue of multiple truths can be addressed either by reporting all counterfactual explanations or by having a criterion to evaluate counterfactuals and select the best one.
-->
反事実は現在のインスタンスとは対照的であり、また選択的である、すなわち通常は少ない特徴量の変化に焦点をあてるため、[人間に優しい説明](#good-explanation)です。
しかし反事実は「羅生門効果」の影響を大きく受けます。
羅生門とは侍の殺人事件が複数人の視点から語られる日本の映画のことです。
それぞれの話は結果を等しく説明しますが、それらは互いには全く異なっているのです。
たいていは、複数の異なった反事実的説明が存在するため、同じことが反事実的説明にも言えます。
それぞれの反事実は、どのように特定の結果に到達するかの異なる「ストーリー」を語っています。
ある反事実は特徴量 A を変化させると言い、一方で、他の反事実は特徴量 A はそのままで、特徴量 B を変化させると言うかもしれませんが、これは矛盾しています。
この複数の真実が存在する問題は、全ての反事実を報告するか、反事実を評価する基準を持ち最良のものを選ぶかによって解決できます。

<!--
Speaking of criteria, how do we define a good counterfactual explanation?
First, the user of a counterfactual explanation defines a relevant change in the prediction of an instance (= the alternative reality), so an obvious first requirement is that **a counterfactual instance produces the predefined prediction as closely as possible**.
It is not always possible to match the predefined output exactly.
In a classification setting with two classes, a rare class and a frequent class, the model could always classify an instance as the frequent class.
Changing the feature values so that the predicted label would flip from the common class to the rare class might be impossible.
We therefore want to relax the requirement that the predicted output of the counterfactual must correspond exactly to the defined outcome.
In the classification example, we could look for a counterfactual where the predicted probability of the rare class is increased to 10% instead of the current 2%.
The question then is, what are the minimum changes to the features so that the predicted probability changes from 2% to 10% (or close to 10%)?
Another quality criterion is that **a counterfactual should be as similar as possible to the instance regarding feature values**.
This requires a distance measure between two instances.
The counterfactual should not only be close to the original instance, but should also **change as few features as possible**.
This can be achieved by selecting an appropriate  distance measure like the Manhattan distance.
The last requirement is that **a counterfactual instance should have feature values that are likely**.
It would not make sense to generate a counterfactual explanation for the rent example where the size of an apartment is negative or the number of rooms is set to 200.
It is even better when the counterfactual is likely according to the joint distribution of the data, e.g. an apartment with 10 rooms and 20 m^2^ should not be regarded as counterfactual explanation.
-->
では、反事実的説明の良し悪しはどのように定めれば良いのでしょうか。
まず、反事実的説明ではユーザーがインスタンス（現実の代替）の予測の変化を定義することから、第一に**反事実的なインスタンスの予測が、事前に定義された予測を可能な限り忠実に再現している**ことが必要です。
定義通りの予測を正確に出力することは常に可能であるとは限りません。
滅多に発生しないクラスと発生する頻度の高いクラスを持つような分類問題では、モデルは常にインスタンスを頻繁に起こるクラスに分類してしまうかもしれません。
このような場合、特徴量を変更することで予測されるラベルを普遍的なクラスから希少なクラスに変えることは不可能かもしれません。
このため、反事実に対する予測は定義された結果に正確に一致していなければならないという条件をもう少し緩めてみましょう。
先の分類問題を例に取ると、希少なクラスが予測される確率を現在の 2% から 10% に増加するような反事実をあげることができます。
このときの質問は、予測される確率を 2% から 10% （もしくは 10% に近い値）に変えるような特徴量に対する最小限の変更は何か、となります。
もう1つの品質に対する基準は**反事実は元のインスタンスの特徴量と可能な限り似ている必要がある**ということです。
これには2つのインスタンス間の距離を測る必要があります。
また、反事実には元のインスタンスに近いだけでなく、**変更された特徴量の数が少ない**ことも必要です。
これはマンハッタン距離などの適切な尺度を選ぶことにより実現できます。
最後の条件は、**反事実的インスタンスはもっともらしい特徴量の値を持つことが必要である**、ということです。
アパートのサイズが負の値であったり、部屋の数が200もあるようなアパートの家賃に対しては、反事実的説明は意味をなさないでしょう。
反事実がデータの同時分布に従っていると、より良いです。例えば、20m^2^ のサイズの部屋が 10 部屋あるアパートは、反事実的説明に用いるべきではありません。

<!--
### Generating Counterfactual Explanations
-->
### 反事実的説明の生成

<!--
A simple and naive approach to generating counterfactual explanations is searching by trial and error.
This approach involves randomly changing feature values of the instance of interest and stopping when the desired output is predicted.
Like the example where Anna tried to find a version of her apartment for which she could charge more rent.
But there are better approaches than trial and error.
First, we define a loss function that takes as input the instance of interest, a counterfactual and the desired (counterfactual) outcome.
The loss measures how far the predicted outcome of the counterfactual is from the predefined outcome and how far the counterfactual is from the instance of interest.
We can either optimize the loss directly with an optimization algorithm or by searching around the instance, as suggested in the "Growing Spheres" method (see [Software and Alternatives](#example-software)).
-->
反事実的説明を生成するためのシンプルかつ素朴な方法は試行錯誤を繰り返しながら探索することです。
この例としては、先のアパートの例で家賃を高くできるアパートの条件を見つけようとしたように、関心のあるインスタンスの特徴量の値をランダムに変え、望む結果が得られたらやめる、といった方法があります。
しかし、試行錯誤して探すよりも良い方法があります。
まず、関心のあるインスタンス、それに対する反事実、そして望む（反事実的な）結果を入力とする損失関数を定義します。
この損失関数は反事実から予測された結果が事前に定義された結果からどの程度離れているか、また反事実そのものがどの程度関心のあるインスタンスから離れているかを測ります。
この損失関数は、直接最適化アルゴリズムを用いるか、"Growing Spheres" という手法で提案されているように、インスタンスの周囲を探索することで最適化できます（詳しくは[ソフトウェア及びその代替手法](#example-software)を参照）。

<!--
In this section, I will present the approach suggested by Wachter et. al (2017)[^wachter].
They suggest minimizing the following loss.
-->
ここでは、2017年、Wacherらにより提案されたアプローチを紹介します。
このアプローチでは次の損失を最小化することを提案しています。

$$L(x,x^\prime,y^\prime,\lambda)=\lambda\cdot(\hat{f}(x^\prime)-y^\prime)^2+d(x,x^\prime)$$

<!--
The first term is the quadratic distance between the model prediction for the counterfactual x' and the desired outcome y', which the user must define in advance.
The second term is the distance d between the instance x to be explained and the counterfactual x', but more about this later.
The parameter $\lambda$ balances the distance in prediction (first term) against the distance in feature values (second term).
The loss is solved for a given $\lambda$ and returns a counterfactual x'.
A higher value of $\lambda$ means that we prefer counterfactuals that come close to the desired outcome y', a lower value means that we prefer counterfactuals x' that are very similar to x in the feature values.
If $\lambda$ is very large, the instance with the prediction that comes closest to y' will be selected, regardless how far it is away from x.
Ultimately, the user must decide how to balance the requirement that the prediction for the counterfactual matches the desired outcome with the requirement that the counterfactual is similar to x.
The authors of the method suggest instead of selecting a value for $\lambda$ to select a tolerance $\epsilon$ for how far away the prediction of the counterfactual instance is allowed to be from y'.
This constraint can be written as:
-->
この式の最初の項では反事実 x' のモデルの予測と、事前にユーザが定義した望ましい結果 y' の二乗距離を計算しています。
第2項は説明されるインスタンス x と反事実 x' の距離を表します。
この項についての詳細は後ほどします。
パラメータ $\lambda$ は第1項目の予測に対する距離と第2項目の特徴量に対する距離のバランスを決定します。
この損失関数は、与えられた $\lambda$ の値に対して、反事実 x' を求めます。
$\lambda$ の値が大きいとき、得られる反事実が求めたい結果 y' により近くなる結果が得られ、$\lambda$ の値が小さいとき、得られる反事実の特徴量が x と近い結果が得られます。
$\lambda$ が非常に大きい値である場合、x からの距離に関わらず y' に最も近い予測をもつインスタンスが選択されます。
最終的には、ユーザは反事実の予測が、どの程度求める結果と一致するかという点と、反事実がどの程度 x と一致するかという点に関して、バランスを決定する必要があります。
この手法の著者は、$\lambda$ の値を決定する代わりに、どれだけ反事実インスタンスの予測が y' と離れていても良いかを表す許容量 $\epsilon$ を決定することを提案しています。
この条件は以下のように記述できます。

$$|\hat{f}(x^\prime)-y^\prime|\leq\epsilon$$

<!--
To minimize this loss function, any suitable optimization algorithm can be used, e.g. Nelder-Mead.
If you have access to the gradients of the machine learning model, you can use gradient-based methods like ADAM.
The instance x to be explained, the desired output y' and the tolerance parameter $\epsilon$ must be set in advance.
The loss function is minimized for x' and the (locally) optimal counterfactual x' returned while increasing $\lambda$ until a sufficiently close solution is found (= within the tolerance parameter).
-->
この損失関数を最小化するには、Nelder-Mead法といった多くの最適化アルゴリズムを用いることができます。
もし機械学習モデルの勾配にアクセスできる場合、ADAM などの勾配ベースの手法を用いることもできます。
説明されるインスタンス x 、求めたい結果 y' 、許容量 $\epsilon$ は事前に定めておく必要があります。
$\lambda$ の値を増加させながら（許容パラメータ内において）十分に近い解を探索することで、損失関数は x' について最小化され、（局所的に）最適な反事実 x' が返されます。

$$\arg\min_{x^\prime}\max_{\lambda}L(x,x^\prime,y^\prime,\lambda)$$

<!--
The function d for measuring the distance between instance x and counterfactual x' is the Manhattan distance weighted feature-wise with the inverse median absolute deviation (MAD).
-->
インスタンス x と反事実 x' の距離を測る関数 d は、中央絶対偏差（median absolute deviation, MAD）の逆数によって特徴量ごとにスケーリングされたマンハッタン距離で表されます。

$$d(x,x^\prime)=\sum_{j=1}^p\frac{|x_j-x^\prime_j|}{MAD_j}$$

<!--
The total distance is the sum of all p feature-wise distances, that is, the absolute differences of feature values between instance x and counterfactual x'.
The feature-wise distances are scaled by the inverse of the median absolute deviation of feature j over the dataset defined as:
-->
距離の合計値は全ての p 個の特徴量間の距離の総和、すなわちインスタンス x と反事実 x' の特徴量の差の絶対値を表します。
特徴量ごとの距離は、データセット内の特徴量 j の MAD の逆数によってスケーリングされます。

$$MAD_j=\text{median}_{i\in{}\{1,\ldots,n\}}(|x_{i,j}-\text{median}_{l\in{}\{1,\ldots,n\}}(x_{l,j})|)$$

<!--
The median of a vector is the value at which half of the vector values are greater and the other half smaller.
The MAD is the equivalent of the variance of a feature, but instead of using the mean as the center and summing over the square distances, we use the median as the center and sum over the absolute distances.
The proposed distance function has the advantage over the Euclidean distance that it introduces sparsity.
This means that two points are closer to each other when fewer features are different.
And it is more robust to outliers.
Scaling with the MAD is necessary to bring all the features to the same scale -- it should not matter whether you measure the size of an apartment in square meters or square feet.
-->
ベクトルの中央値はベクトル内の値の半分がこれより大きくなり、残りの半分がこれより小さくなる値を指します。
MAD は特徴量の分散に相当しますが、平均を中心として距離の二乗和をとる代わりに、中央値を中心として距離の絶対値の和を用います。
ユークリッド距離と比較すると、提案された距離関数はスパース性が導入されているという利点があります。
これは、異なる特徴量の数が少ないとき、2点がより近くなることを意味します。
また、外れ値に対してよりロバストになります。
MAD によりスケーリングすることは全ての特徴量を同じスケールにするためことに必要です。
アパートの大きさをメートルで測るかフィートで測るかは結果に影響すべきではありません。

<!--
The recipe for producing the counterfactuals is simple:
-->
反事実を生成する手順はシンプルです。

<!--
1. Select an instance x to be explained, the desired outcome y', a tolerance $\epsilon$ and a (low) initial value for $\lambda$.
1. Sample a random instance as initial counterfactual.
1. Optimize the loss with the initially sampled counterfactual as starting point.
1. While $|\hat{f}(x^\prime)-y^\prime|>\epsilon$:
    - Increase $\lambda$.
    - Optimize the loss with the current counterfactual as starting point.
    - Return the counterfactual that minimizes the loss.
1. Repeat steps 2-4 and return the list of counterfactuals or the one that minimizes the loss.
-->

1. 説明したいインスタンス x、望ましい結果 y'、許容量 $\epsilon$、$\lambda$ の初期値（小さな値）を選択する。
1. 最初に用いる反事実としてランダムにインスタンスをサンプリングする。
1. 最初に選んだ反事実を出発点として損失関数を最適化する。
1. $|\hat{f}(x^\prime)-y^\prime|>\epsilon$ を満たす間、
    - $\lambda$ の値を増加させる。
    - 現在の反事実を出発点として損失を最適化する。
    - 損失を最小化する反事実を返す。
1. ステップ 2-4 を繰り返し、反事実のリストもしくは損失を最小化するものを返す。

<!--
### Examples
-->
### 例

<!--
Both examples are from the work of Wachter et. al (2017).
-->
どちらの例も Wachter らの論文 (2017)より引用しています。

<!--
In the first example, the authors train a three-layer fully-connected neural network to predict a student's average grade of the first year at law school, based on grade point average (GPA) prior to law school, race and law school entrance exam scores.
The goal is to find counterfactual explanations for each student that answer the following question:
How would the input features need to be changed, to get a predicted score of 0?
Since the scores have been normalized before, a student with a score of 0 is as good as the average of the students.
A negative score means a below-average result, a positive score an above-average result.
-->
最初の例では、著者は3層の全結合型ニューラルネットを学習し、法科大学入学前の成績（GPA）、人種、入学試験の得点から、入学1年目の学生の成績を予測しようとしています。
目標は次の問いを満たす反事実的説明を各学生に対して求めることです。
予測スコアが 0 となるにはどのように入力特徴量を変更する必要があるでしょうか。
スコアは事前に標準化されているため、スコアが 0 というのはその学生が平均的であることを意味します。
また、スコアが負であることは平均以下、スコアが正であることは平均以上であることを意味します。

<!--
The following table shows the learned counterfactuals:
-->
次の表は学習された反事実を表しています。

<!--
| Score | GPA | LSAT | Race | GPA x' | LSAT x' |  Race x'|
| ------|--------------| --------------| --------------| -------| --------| ------- |
| 0.17 | 3.1 | 39.0 | 0 | 3.1 | 34.0 | 0|
| 0.54 | 3.7 | 48.0 | 0 | 3.7 | 32.4 | 0|
| -0.77| 3.3 | 28.0 | 1 | 3.3 | 33.5 | 0|
| -0.83| 2.4 | 28.5 | 1 | 2.4 | 35.8 | 0|
| -0.57| 2.7 | 18.3 | 0 | 2.7 | 34.9 | 0|
-->

| 点数 | GPA | 入学試験の点数 | 人種 | GPA x' | 入学試験の点数 x' |  人種 x'|
| ------|--------------| --------------| --------------| -------| --------| ------- |
| 0.17 | 3.1 | 39.0 | 0 | 3.1 | 34.0 | 0|
| 0.54 | 3.7 | 48.0 | 0 | 3.7 | 32.4 | 0|
| -0.77| 3.3 | 28.0 | 1 | 3.3 | 33.5 | 0|
| -0.83| 2.4 | 28.5 | 1 | 2.4 | 35.8 | 0|
| -0.57| 2.7 | 18.3 | 0 | 2.7 | 34.9 | 0|

<!--
The first column contains the predicted score, the next 3 columns the original feature values and the last 3 columns the counterfactual feature values that result in a score close to 0.
The first two rows are students with above-average predictions, the other three rows below-average.
The counterfactuals for the first two rows describe how the student features would have to change to decrease the predicted score and for the other three cases how they would have to change to increase the score to the average.
The counterfactuals for increasing the score always change the race from black (coded with 1) to white (coded with 0) which shows a racial bias of the model.
The GPA is not changed in the counterfactuals, but LSAT is.
-->
最初の列は予測した点数、次の3列は特徴量の元々の値、そして最後の3列が点数が0に近い値となるような反事実の特徴量の値を表しています。
また、最初の2行は平均以上の予測となった学生、残りの3行は平均以下となった学生を表しています。
ここで、最初の2行の反事実は、予測値が下がるには各生徒の特徴量がどのように変更される必要があるのか、残りの3行では予測値が上がるためには各生徒の特徴量がどのように変更される必要があるのかを示しています。
点数が上がる反事実では常に人種が黒人（1の値）から白人（0の値）となっており、モデルが人種に対するバイアスを持っていることがわかります。
この時、GPAは変化していませんが、入学試験の点数が変化していることがわかります。

<!--
The second example shows counterfactual explanations for predicted risk of diabetes.
A three-layer fully-connected neural network is trained to predict the risk for diabetes depending on age, BMI, number of pregnancies and so on for women of Pima heritage.
The counterfactuals answer the question: Which feature values must be changed to increase or decrease the risk score of diabetes to 0.5?
The following counterfactuals were found:
-->
2つ目の例では糖尿病のリスクの予測に対する反事実的説明を表しています。
3層全結合ニューラルネットを訓練し、年齢、BMI、妊娠の回数などからピマ族の血を引く女性が糖尿病にかかるリスクを予測しています。
反事実を次の問いとして、糖尿病リスクのスコアを0.5に増加させる、もしくは減少させるにはどの特徴量が変更される必要があるかを考えます。

<!--
- Person 1: If your 2-hour serum insulin level was 154.3, you would have a score of 0.51
- Person 2: If your 2-hour serum insulin level was 169.5, you would have a score of 0.51
- Person 3: If your Plasma glucose concentration was 158.3 and your 2-hour serum insulin level was 160.5, you would have a score of 0.51
-->
- 1人目：2時間の血中インスリン測定の結果が 154.3 ならば、スコアが0.51となる
- 2人目：2時間の血中インスリン測定の結果が 169.5 ならば、スコアが0.51となる
- 3人目：血糖値が 158.3 かつ2時間の血中インスリン測定の結果が 160.5 ならば、スコアが0.51となる

<!--
### Advantages
-->
### 長所

<!--
**The interpretation of counterfactual explanations is very clear**.
If the feature values of an instance are changed according to the counterfactual, the prediction changes to the predefined prediction.
There are no additional assumptions and no magic in the background.
This also means it is not as dangerous as methods like [LIME](#lime), where it is unclear how far we can extrapolate the local model for the interpretation.
-->
**反事実的説明は単純明快です。**
反事実に従ってあるインスタンスの特徴量を改変すると、予測結果が予め設定した値に変わります。
背景には他の仮定も魔法もありません。
従って、LIMEのように局所的なモデルをどれだけ外挿していいか不明な手法と比べると、危険性が低いです。

<!--
The counterfactual method creates a new instance, but we can also summarize a counterfactual by reporting which feature values have changed.
This gives us **two options for reporting our results**.
You can either report the counterfactual instance or highlight which features have been changed between the instance of interest and the counterfactual instance.
-->
反事実的説明は新たなインスタンスを生成しますが、反事実がどの特徴量を変更したものか要約できます。
結果をまとめるにあたって2つの選択肢が生じます。
反事実インスタンスを報告するか、説明したいインスタンスとその反事実インスタンスとの間でどの特徴量が異なるかを示すか選べます。

<!--
The **counterfactual method does not require access to the data or the model**.
It only requires access to the model's prediction function, which would also work via a web API, for example.
This is attractive for companies which are audited by third parties or which are offering explanations for users without disclosing the model or data.
A company has an interest in protecting model and data because of trade secrets or data protection reasons.
Counterfactual explanations offer a balance between explaining model predictions and protecting the interests of the model owner.
-->
反事実的手法はデータやモデルへのアクセスが不要です。
モデルの予測関数にさえアクセスできればよく、web APIなどによる運用も可能です。
この性質は、サードパーティの監査を受ける企業やデータやモデルを非公開にしたままユーザーに説明性を提供したい企業にとって魅力的です。
会社は企業秘密やデータ保護の観点から、モデルやデータの秘匿に関心を寄せています。
反事実による説明は、モデルの予測を説明することと、モデルの所有者の利益を守ることのバランスを取ることができます。

<!--
The method **works also with systems that do not use machine learning**.
We can create counterfactuals for any system that receives inputs and returns outputs.
The system that predicts apartment rents could also consist of handwritten rules, and counterfactual explanations would still work.
-->
反事実的手法は機械学習を用いないシステムにも応用可能です。
入出力を持つ任意のシステムに対して反事実は作れます。
アパートの賃料予測は手書きのルールによる構築も可能ですが、反事実的説明は依然として機能します。

<!--
**The counterfactual explanation method is relatively easy to implement**, since it is essentially a loss function that can be optimized with standard optimizer libraries.
Some additional details must be taken into account, such as limiting feature values to meaningful ranges (e.g. only positive apartment sizes).
-->

**反事実的説明は比較的簡単に実装できます。**
というのも、反事実的説明は標準的な最適化ライブラリによって最適化可能な損失関数にすぎないからです。
ただし、特徴量の範囲の妥当性などいくつか追加で考慮すべき点があります（例えば
、アパートの広さは正の値しか取りません）。

<!--
### Disadvantages
-->
### 短所

<!--
**For each instance you will usually find multiple counterfactual explanations (Rashomon effect)**.
This is inconvenient -- most people prefer simple explanations over the complexity of the real world.
It is also a practical challenge.
Let us say we generated 23 counterfactual explanations for one instance.
Are we reporting them all?
Only the best?
What if they are all relatively "good", but very different?
These questions must be answered anew for each project.
It can also be advantageous to have multiple counterfactual explanations, because then humans can select the ones that correspond to their previous knowledge.
-->
**通常、各インスタンスは複数の反事実的説明を持ちます（羅生門効果）**。
これは便利ではありません。多くの人は現実世界の複雑さよりも簡単な説明の方を好みます。
また、これは実用面での課題でもあります。
例として1つのインスタンスに対して23個の反事実的説明が生成されたとしましょう。
これらを全て報告すべきでしょうか、それとも最も良いものだけにするべきでしょうか。
もしそれらが全て比較的「良い」もので、中身が異なるものであったならどうでしょうか。
これらの課題はプロジェクトごとに新たに答えを出さなくてはいけません。
過去の経験に合うものを選択できるという理由から、複数の反事実的説明を持つことが有利になることもありえます。

<!--
There is **no guarantee that for a given tolerance $\epsilon$ a counterfactual instance is found**.
That is not necessarily the fault of the method, but rather depends on the data.
-->
**与えられた許容量 $\epsilon$ に対して、反事実的説明が見つかるとは限りません**。
これは手法による過失ではなく、データに依存しています。

<!--
The proposed method **does not handle categorical features** with many different levels well.
The authors of the method suggested running the method separately for each combination of feature values of the categorical features, but this will lead to a combinatorial explosion if you have multiple categorical features with many values.
For example, 6 categorical features with 10 unique levels would mean 1 million runs.
A solution for only categorical features was proposed by Martens et. al (2014)[^martens].
A solution that handles both numerical and categorical variables with a principled way of generating perturbations for categorical variables is implemented in the Python package [Alibi](https://docs.seldon.io/projects/alibi/en/stable/methods/CFProto.html).
-->
提案された手法では異なるレベルを持つカテゴリカル特徴量を上手く扱うことができません。
この手法の著者はカテゴリカル特徴量の組み合わせごとにこの方法を実行することを提案していますが、これも多くの値を持つ複数のカテゴリカル特徴量を持つ場合には組合せ爆発を起こしてしまいます。
例えば10個の固有の値を持つカテゴリカル特徴量が6個あった場合、100万回もの実行が必要となります。
カテゴリカル特徴量のみを扱う際の解決策としては、Martensらが提案した手法(2014)[^martens] が存在します。
また、量的変数とカテゴリカル変数を両方扱い、カテゴリカル変数に対して原理的に摂動を生成する手法が Python パッケージ [Alibi](https://docs.seldon.io/projects/alibi/en/stable/methods/CFProto.html) に実装されています。

<!--
### Software and Alternatives {#example-software}
-->
### ソフトウェアと代替手法

<!--
Counterfactuals explanations are implemented in the Python package [Alibi](https://github.com/SeldonIO/alibi). Authors of the package implement a [simple counterfactual method](https://docs.seldon.io/projects/alibi/en/stable/methods/CF.html) as well as an [extended method](https://docs.seldon.io/projects/alibi/en/stable/methods/CFProto.html) that uses class prototypes to improve the interpretability and convergence of the algorithm outputs[^vanlooveren].
-->
反事実的説明のPython実装は [Alibi](https://github.com/SeldonIO/alibi) パッケージです。パッケージの作者は [単純な反事実的手法](https://docs.seldon.io/projects/alibi/en/stable/methods/CF.html) と [その拡張](https://docs.seldon.io/projects/alibi/en/stable/methods/CFProto.html) としてプロトタイプクラスを用いて解釈しやすくし、アルゴリズムの収束性を改善したものを実装しています。


<!--
A very similar approach was proposed by Martens et. al (2014) for explaining document classifications.
In their work, they focus on explaining why a document was or was not classified as a particular class.
The difference to the method presented in this chapter is that Martens et. al (2014) focus on text classifiers, which have word occurrences as inputs.
-->
Martens らは文書の分類問題に対して同様の手法を提案しています(2014)。
彼らはある文書がどうして特定のクラスに分類されなかったのか説明することに注力しています。
本章で説明した手法と異なり、Martens らは文書の分類に注目しているので、入力は単語の出現頻度です。

<!--
An alternative way to search counterfactuals is the Growing Spheres algorithm by Laugel et. al (2017)[^spheres].
The method first draws a sphere around the point of interest, samples points within that sphere, checks whether one of the sampled points yields the desired prediction, contracts or expands the sphere accordingly until a (sparse) counterfactual is found and finally returned.
They do not use the word counterfactual in their paper, but the method is quite similar.
They also define a loss function that favors counterfactuals with as few changes in the feature values as possible.
Instead of directly optimizing the function, they suggest the above-mentioned search with spheres.
-->
他にも反事実を探す方法として、Laugel らによるGrowing Spheresアルゴリズム (2017)[^spheres]があります。
この手法は、注目したい点の周囲に球を描き、その球に含まれる点を抽出し、抽出した点の予測結果が期待通りか確認しながら、（スパースな）反事実を発見し提示するまで球を拡大していきます。
彼らは論文中で反事実 (counterfactual) という用語を用いていませんが、手法としてはよく似ています。
彼らはさらに特徴量を最小限に改変して反事実を得るための損失関数を定義しています。
損失関数を直接最適化する代わりに、彼らは上述の球を用いた探索を提案しています。

<!--
Anchors by Ribeiro et. al (2018)[^anchors] are the opposite of counterfactuals, see chapter about [Scoped Rules (Anchors)](#anchors).
-->
Ribeiro らによるAnchor (2018)[^anchors]は反事実とは対極的な存在です。
これについては、[Scoped Rules (Anchors)](#anchors)の章をご覧下さい。

[^martens]: Martens, David, and Foster Provost. "Explaining data-driven document classifications." (2014).

[^anchors]: Ribeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. "Anchors: High-precision model-agnostic explanations." AAAI Conference on Artificial Intelligence (2018).

[^spheres]: Laugel, Thibault, et al. "Inverse classification for comparison-based interpretability in machine learning." arXiv preprint arXiv:1712.08443 (2017).

[^wachter]: Wachter, Sandra, Brent Mittelstadt, and Chris Russell. "Counterfactual explanations without opening the black box: Automated decisions and the GDPR." (2017).

[^vanlooveren]: Van Looveren, Arnaud, and Janis Klaise. "Interpretable Counterfactual Explanations Guided by Prototypes."  arXiv preprint arXiv:1907.02584 (2019).
